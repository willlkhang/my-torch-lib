{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMALosRq9CLd3LL+VXJ/z9l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OobIOcuJ4oM1","executionInfo":{"status":"ok","timestamp":1763385139559,"user_tz":-480,"elapsed":24277,"user":{"displayName":"Minh Khang Nguyen","userId":"17151079270851040988"}},"outputId":"1548cf0a-28bf-46f2-f303-686786eba50e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["zip_folder = '/content/drive/My Drive/Deep Learning Machine Learning/data.zip'\n","extract_folder = '/content/drive/My Drive/Deep Learning Machine Learning/data/'"],"metadata":{"id":"qoVWmbBt6Qlj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2 as cv\n","import os\n","from PIL import Image\n","\n","from torch import nn\n","\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","\n","from torch.utils.tensorboard import SummaryWriter\n","\n","import torch.optim as optim"],"metadata":{"id":"bTTYqMPI7qv2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import zipfile\n","import pandas as pd\n","\n","def extract_zip(zip_path, extract_path):\n","      if not os.path.exists(extract_path):\n","          os.makedirs(extract_path)\n","      with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","          zip_ref.extractall(extract_path)"],"metadata":{"id":"PcuTINHe69X0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_path = os.path.join(extract_folder + \"data\")\n","data_dir = os.listdir(data_path)\n","print(data_dir)\n","\n","items = os.listdir(os.path.join((extract_folder + \"data\"), \"train\"))\n","label_dict = {category: idx for idx, category in enumerate(items)}\n","print(label_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yjj5nuFG7e55","executionInfo":{"status":"ok","timestamp":1763387068433,"user_tz":-480,"elapsed":5,"user":{"displayName":"Minh Khang Nguyen","userId":"17151079270851040988"}},"outputId":"d7ce263a-2b90-417e-e827-910e9ae4c228"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['test', 'train', 'valid']\n","{'accessories': 0, 'jackets': 1, 'jeans': 2, 'knitwear': 3, 'shirts': 4, 'shoes': 5, 'shorts': 6, 'tees': 7}\n"]}]},{"cell_type":"code","source":["class Data_class(Dataset):\n","    def __init__(self, label_map, set_path, labels=None, transform=None, test=False):\n","\n","      self.label_map = label_map\n","      self.labels = labels\n","      self.set_path = set_path\n","\n","      self.transform = transform\n","\n","      self.test = test\n","      self.data = self.data_loader()\n","\n","    def data_loader(self):\n","      images = []\n","\n","      if self.test:\n","        img_files = os.listdir(self.set_path)\n","        for img_file in img_files:\n","          print(f\"loading image {img_file}...\")\n","          img_path = os.path.join(self.set_path, img_file)\n","          try:\n","            img = Image.open(img_path).convert('RGB')\n","            images.append(img)\n","          except Exception as e:\n","            print(f\"Error loading image {img_path}: {e}\")\n","        return images\n","\n","      else:\n","        for item in self.labels:\n","          category_path = os.path.join(self.set_path, item)\n","\n","          label = self.label_map[item]\n","          print(f'Loading {item}...')\n","\n","          for img_file in os.listdir(category_path):\n","            img_path = os.path.join(category_path, img_file)\n","\n","            try:\n","              img = Image.open(img_path).convert('RGB')\n","\n","              if self.test == False:\n","                images.append((img, label))\n","              else:\n","                images.append(img)\n","\n","            except Exception as e:\n","              print(f\"Error loading image {img_path}: {e}\")\n","        return images\n","\n","    def __len__(self):\n","      return len(self.data)\n","\n","    def __getitem__(self, idx):\n","      if(self.test):\n","        image = self.data[idx]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image\n","\n","      else:\n","        image, label = self.data[idx]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label"],"metadata":{"id":"OVUqJdGx7oqQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Trainer:\n","  def __init__(self, model, train_loader, val_loader, criterion, optimizer, num_epochs=20):\n","\n","    #training device variables group\n","    if torch.cuda.is_available():\n","      self.device = torch.device('cuda')\n","    else:\n","      self.device = torch.device('cpu')\n","    self.model = model.to(self.device)\n","\n","    #data loader variables group\n","    self.train_loader = train_loader\n","    self.val_loader = val_loader\n","\n","    #training variables group\n","    self.criterion = criterion\n","    self.optimizer = optimizer\n","    self.num_epochs = num_epochs\n","\n","    #variabels for tracking and plotting\n","    self.train_losses = []\n","    self.val_losses = []\n","    self.train_accuracies = []\n","    self.val_accuracies = []\n","\n","    # TensorBoard writer\n","    self.writer = SummaryWriter()\n","\n","  def fit(self):\n","    for epoch in range(self.num_epochs):\n","\n","      train_loss, train_accuracy = self.train()\n","      val_loss, val_accuracy = self.evaluate()\n","\n","      # Store loss and accuracy values\n","      self.train_losses.append(train_loss)\n","      self.train_accuracies.append(train_accuracy)\n","      self.val_losses.append(val_loss)\n","      self.val_accuracies.append(val_accuracy)\n","\n","      # Log metrics to TensorBoard\n","      self.writer.add_scalar('Loss/Train', train_loss, epoch)\n","      self.writer.add_scalar('Loss/Validation', val_loss, epoch)\n","      self.writer.add_scalar('Accuracy/Train', train_accuracy, epoch)\n","      self.writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)\n","      print(f'Epoch [{epoch+1}/{self.num_epochs}], '\n","            f' Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%,'\n","            f' Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n","\n","      self.writer.close() # Close the writer after training is complete\n","\n","  def train(self):\n","    run_loss = 0.0\n","    correct_pred = 0\n","    total_samples = 0\n","\n","    self.model.train()  # Set model to training mode\n","\n","    for feature, labels in self.train_loader:\n","      feature, labels = feature.to(self.device), labels.to(self.device)\n","\n","      self.optimizer.zero_grad()\n","      outputs = self.model(feature)\n","      loss = self.criterion(outputs, labels)#next 3 lines are backpropagration step\n","      loss.backward()\n","      self.optimizer.step()\n","\n","      # Track loss and accuracy\n","      run_loss += loss.item()\n","      _, predicted = torch.max(outputs, 1)\n","      correct_pred += (predicted == labels).sum().item()\n","      total_samples += labels.size(0)\n","\n","    # Calculate training loss and accuracy\n","    train_loss = run_loss / len(self.train_loader)\n","    train_accuracy = correct_pred / total_samples * 100\n","\n","    return train_loss, train_accuracy\n","\n","  @torch.no_grad()\n","  def evaluate(self):\n","    val_loss = 0.0\n","    correct_pred = 0\n","    total_samples = 0\n","\n","    self.model.eval()  # Set model to evaluation mode\n","    for images, labels in self.val_loader:\n","      images, labels = images.to(self.device), labels.to(self.device)\n","\n","      # Forward pass\n","      outputs = self.model(images)\n","      loss = self.criterion(outputs, labels)\n","      val_loss += loss.item()\n","\n","      # Track accuracy\n","      _, predicted = torch.max(outputs, 1)\n","      correct_pred += (predicted == labels).sum().item()\n","      total_samples += labels.size(0)\n","\n","    # Calculate validation loss and accuracy\n","    val_loss = val_loss / len(self.val_loader)\n","    val_accuracy = correct_pred / total_samples * 100\n","\n","    return val_loss, val_accuracy"],"metadata":{"id":"4hDoi5OK8xW1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n","    transforms.RandomRotation(20),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","testVal_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","val_set = os.path.join(data_path, \"valid\")\n","train_set = os.path.join(data_path, \"train\")\n","test_set = os.path.join(data_path, \"test/unknown\")\n","\n","print(\"Loading test set:\")\n","test_dataset = Data_class(label_dict, test_set, transform=testVal_transform, test=True)\n","\n","print(\"\\nLoading validaiton set: \")\n","val_dataset = Data_class(label_dict, val_set, items, transform=testVal_transform)\n","\n","print(\"\\nLoading train set: \")\n","train_dataset = Data_class(label_dict, train_set, items, transform=train_transform)\n","\n","print(\"\\n\\nFinish uploading\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ixzhhtOk86tR","outputId":"ec6a8acf-5881-490c-8da8-9146edbb06a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading test set:\n","loading image test1.jpg...\n","loading image test2.jpg...\n","loading image test3.jpg...\n","loading image test4.jpg...\n","loading image test5.jpg...\n","loading image test6.jpg...\n","loading image test7.jpg...\n","loading image test8.jpg...\n","\n","Loading validaiton set: \n","Loading accessories...\n","Loading jackets...\n","Loading jeans...\n","Loading knitwear...\n","Loading shirts...\n"]}]},{"cell_type":"code","source":["batch_size = 32\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","print(\"DataLoader objects created successfully.\")"],"metadata":{"id":"naUCeO_k8-Jp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MLP(nn.Module):\n","\n","  def __init__(self, inputSize, outputSize, dropout=0.0):\n","    super().__init__()\n","\n","    self.net = nn.Sequential(\n","        nn.Flatten(),\n","        nn.Linear(inputSize, 258),\n","        nn.ReLU(inplace=True),\n","        nn.Linear(258, 128),\n","        nn.ReLU(inplace=True),\n","        nn.Linear(128, 64),\n","        nn.ReLU(inplace=True),\n","        nn.Linear(64, outputSize),\n","        nn.ReLU(inplace=True)\n","    )\n","\n","    if torch.cuda.is_available():\n","        self.cuda()\n","\n","  def forward(self, X):\n","    return self.net(X)\n"],"metadata":{"id":"Tx6cBA2E9A4X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_dim = 224 * 224 * 3\n","output_dim = 8\n","\n","mlp_model = MLP(inputSize=input_dim, outputSize=output_dim, dropout=0.02)\n","\n","if torch.cuda.is_available():\n","    mlp_model.cuda()\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(mlp_model.parameters(), lr=1e-4)\n","\n","\n","mlp_trainer = Trainer(mlp_model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs=15)\n","mlp_trainer.fit()"],"metadata":{"id":"98FPOwSQACIl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["impt matplotlib.pyplot as plt\n","from matplotlib.ticker import MaxNLocator\n","\n","def plot_afterTraining(trainer_name, model_name):\n","  epochs = range(1, trainer_name.num_epochs + 1)\n","  plt.figure(figsize=(12, 5))\n","\n","  plt.suptitle(model_name, fontsize=16)\n","\n","  loss_plot(epochs, trainer_name)\n","  accuracy_plot(epochs, trainer_name)\n","\n","  plt.tight_layout()\n","\n","  #full_save = os.path.join(save_path, model_name + '-curves')\n","  #plt.savefig(full_save)\n","\n","  plt.show()\n","  plt.close()\n","\n","def loss_plot(epochs, trainer_name):\n","  plt.subplot(1, 2, 1)\n","  plt.plot(epochs, trainer_name.train_losses, 'b', label='Training loss')\n","  plt.plot(epochs, trainer_name.val_losses, 'r', label='Validation loss')\n","  plt.title('Training and Validation Loss')\n","  plt.xlabel('Epochs')\n","  plt.ylabel('Loss')\n","  plt.legend()\n","\n","  ax = plt.gca() #get current axis\n","  ax.xaxis.set_major_locator(MaxNLocator(integer=True)) #only int\n","  ax.yaxis.set_major_locator(MaxNLocator(nbins=8))\n","\n","\n","def accuracy_plot(epochs, trainer_name):\n","  plt.subplot(1, 2, 2)\n","  plt.plot(epochs, trainer_name.train_accuracies, 'b', label='Training accuracy')\n","  plt.plot(epochs, trainer_name.val_accuracies, 'r', label='Validation accuracy')\n","  plt.title('Training and Validation Accuracy')\n","  plt.xlabel('Epochs')\n","  plt.ylabel('Accuracy (%)')\n","  plt.legend()\n","\n","  ax = plt.gca()\n","  ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n","  ax.yaxis.set_major_locator(MaxNLocator(nbins=8))"],"metadata":{"id":"XggoY63YAcSw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_afterTraining(mlp_trainer, \"mlp_trainer\")"],"metadata":{"id":"PGFurxVaDY5R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Q0FAAaebDdvf"},"execution_count":null,"outputs":[]}]}