{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6162e062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b30e9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52292d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conv --> batchnorm --> relu\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, stride, padding, bias=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size, stride, padding, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8344aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dept --> wisepoint\n",
    "class seperableConv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, stride, padding, bias=False):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channel,\n",
    "                                   in_channel,\n",
    "                                   kernel_size, stride, padding,\n",
    "                                   groups=in_channel,\n",
    "                                   bias=bias)\n",
    "        self.pointwise = nn.Conv2d(in_channel, out_channel, 1, 1, 0, bias=bias) #kernel, stride, padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2321e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#residuel block with 1x1 shortcut\n",
    "class EntryFlowBlock(nn.Module):\n",
    "  def __init__(self, in_channel, out_channel):\n",
    "    super().__init__() # Add this line\n",
    "    #shortcut path to make x in h(x) = f(x) + x (explain this later)\n",
    "    self.shortcut_conv = nn.Conv2d(in_channel, out_channel, 1, stride=2, bias=False)\n",
    "    self.shortcut_bn = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "    #main path\n",
    "    self.sep_conv1 = seperableConv(in_channel, out_channel, 3, 1, 1, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(out_channel) # This BatchNorm expects out_channel, which is 128\n",
    "    self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "    self.sep_conv2 = seperableConv(out_channel, out_channel, 3, 1, 1, bias=False)\n",
    "    self.bn2 = nn.BatchNorm2d(out_channel) # This BatchNorm expects out_channel, which is 128\n",
    "\n",
    "    self.pool1 = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    gx = self.shortcut_bn(self.shortcut_conv(x)) #g(x)\n",
    "\n",
    "    #mainpath\n",
    "    fx = self.bn1(self.sep_conv1(x))\n",
    "    fx = self.relu1(fx)\n",
    "    fx = self.bn2(self.sep_conv2(fx))\n",
    "    fx = self.pool1(fx)\n",
    "\n",
    "    return fx + gx #h(x) = f(x) + g(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99ed650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#residule from the middle floW\n",
    "class MiddleFlowBlock(nn.Module):\n",
    "  def __init__(self, channels=728):\n",
    "    super().__init__()\n",
    "\n",
    "    self.relu = nn.ReLU()\n",
    "    self.sep_conv1 = seperableConv(channels, channels, 3, 1, 1, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    self.sep_conv2 = seperableConv(channels, channels, 3, 1, 1, bias=False)\n",
    "    self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    self.sep_conv3 = seperableConv(channels, channels, 3, 1, 1, bias=False)\n",
    "    self.bn3 = nn.BatchNorm2d(channels)\n",
    "\n",
    "  def forward(self, x):\n",
    "    shortcut = x\n",
    "\n",
    "    fx = self.relu(x)\n",
    "    fx = self.bn1(self.sep_conv1(fx))\n",
    "    fx = self.relu(fx)\n",
    "    fx = self.bn2(self.sep_conv2(fx))\n",
    "    fx = self.relu(fx)\n",
    "    fx = self.bn3(self.sep_conv3(fx))\n",
    "\n",
    "    return fx + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d797ee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExitFlowBlock(nn.Module):\n",
    "  def __init__(self, in_channel=728, out_channel=1028):\n",
    "    super().__init__()\n",
    "    #shortcut path g(x)\n",
    "    self.shortcut_conv = nn.Conv2d(in_channel, out_channel, 1, stride=2, bias=False)\n",
    "    self.shortcut_bn = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "    #f(x) path\n",
    "    self.relu = nn.ReLU()\n",
    "    self.sep_conv1 = seperableConv(in_channel, out_channel, 3, 1, 1, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "    self.sep_conv2 = seperableConv(out_channel, out_channel, 3, 1, 1, bias=False)\n",
    "    self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "    self.pool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    gx = self.shortcut_bn(self.shortcut_conv(x))\n",
    "\n",
    "    fx = self.relu(x)\n",
    "    fx = self.bn1(self.sep_conv1(fx))\n",
    "    fx = self.relu(fx)\n",
    "    fx = self.bn2(self.sep_conv2(fx))\n",
    "    fx = self.pool(fx)\n",
    "\n",
    "    return fx + gx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4a2a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sepconv -> bn -> relu\n",
    "class sepconvRelu(nn.Module):\n",
    "  def __init__(self, in_channel, out_channel):\n",
    "    super().__init__()\n",
    "    self.sep_conv = seperableConv(in_channel, out_channel, 3, 1, 1, bias=False)\n",
    "    self.bn = nn.BatchNorm2d(out_channel)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.relu(self.bn(self.sep_conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1292d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_xception(nn.Module):\n",
    "  def __init__(self, output_size, lr=1e-3):\n",
    "    super().__init__()\n",
    "\n",
    "    self.entryflow = nn.Sequential(\n",
    "        ConvBlock(3, 32, 3, 2, 0),\n",
    "        ConvBlock(32, 64, 3, 1, 1),\n",
    "\n",
    "\n",
    "        EntryFlowBlock(64, 128),\n",
    "        EntryFlowBlock(128, 256),\n",
    "        EntryFlowBlock(256, 728)\n",
    "    )\n",
    "\n",
    "    middle_blocks = []\n",
    "    for _ in range(8):\n",
    "      middle_blocks.append(MiddleFlowBlock(channels=728))\n",
    "\n",
    "    self.middleflow = nn.Sequential(*middle_blocks)\n",
    "\n",
    "    self.exitflow = nn.Sequential(\n",
    "        ExitFlowBlock(728, 1024),\n",
    "        sepconvRelu(1024, 1536),\n",
    "        sepconvRelu(1536, 2048)\n",
    "    )\n",
    "\n",
    "    self.glob_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "    self.fc = nn.Linear(2048, output_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.entryflow(x)\n",
    "    x = self.middleflow(x)\n",
    "    x = self.exitflow(x)\n",
    "    x = self.glob_avg_pool(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.fc(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f3e8c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items of ./handsign.zip to ./handsign_local\n",
      "['test', 'README.roboflow.txt', 'data.yaml', 'train', 'valid', 'README.dataset.txt']\n",
      "Found Directory:  handsign_local/test\n",
      "Extracting...\n",
      "Building Data, Found : 1003 / 1003\n",
      "Found Directory:  handsign_local/train\n",
      "Extracting...\n",
      "Building Data, Found : 7355 / 7355\n",
      "Found Directory:  handsign_local/valid\n",
      "Extracting...\n",
      "Building Data, Found : 993 / 993\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "zip_folder_path = './handsign.zip'\n",
    "\n",
    "local_extraction_path = './handsign_local'\n",
    "\n",
    "if os.path.exists(local_extraction_path) is False:\n",
    "  os.makedirs(local_extraction_path, exist_ok=True)\n",
    "\n",
    "  with zipfile.ZipFile(zip_folder_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(local_extraction_path)\n",
    "\n",
    "  print(f\"Items of {zip_folder_path} to {local_extraction_path}\")\n",
    "else:\n",
    "  print(\"Already unzip\")\n",
    "\n",
    "import shutil\n",
    "\n",
    "items = os.listdir(local_extraction_path)\n",
    "for i in items:\n",
    "  if(i == '__MACOSX'):\n",
    "    shutil.rmtree(os.path.join(local_extraction_path, i))\n",
    "\n",
    "items = os.listdir(local_extraction_path) # Refresh\n",
    "print(items)\n",
    "\n",
    "class_labels = ['thumbs-up', 'okay', 'peace', 'palm-up', 'palm-down']\n",
    "\n",
    "def extract_images(PATH: Path):\n",
    "    data = {}\n",
    "    for dir in PATH.iterdir():\n",
    "        if not dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        print('Found Directory: ', dir)\n",
    "        print('Extracting...')\n",
    "        data[dir.name] = {}\n",
    "        dir_dict = data[dir.name]\n",
    "\n",
    "        img_path = dir / 'images'\n",
    "        labels_path = dir / 'labels'\n",
    "\n",
    "        if not img_path.is_dir() or not labels_path.is_dir():\n",
    "            print(f\"  Skipping {dir.name}: 'images' or 'labels' folder missing.\")\n",
    "            continue\n",
    "\n",
    "        label_files = list(labels_path.glob('*.txt'))\n",
    "        if not label_files:\n",
    "            print(f\"  No .txt label files found in {labels_path}\")\n",
    "            continue\n",
    "\n",
    "        log = None\n",
    "        for i, label_file in enumerate(label_files):\n",
    "            file_stem = label_file.stem\n",
    "\n",
    "            # Find the matching image (jpg, png, etc.)\n",
    "            image_pth = next(img_path.glob(f'{file_stem}.*'), None)\n",
    "\n",
    "            if image_pth is None: # If no matching image, skip\n",
    "                continue\n",
    "\n",
    "            try: # If label file is corrupt, skip\n",
    "                with open(label_file, 'r') as f:\n",
    "                    cls = int(f.read().split()[0])\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            if cls not in dir_dict.keys():\n",
    "                dir_dict[cls] = []\n",
    "            dir_dict[cls].append(image_pth)\n",
    "\n",
    "            log = f'Building Data, Found : {i + 1} / {len(label_files)}'\n",
    "            print(log, end='\\r')\n",
    "\n",
    "        if log:\n",
    "            print(log) # Print final count\n",
    "    return data\n",
    "\n",
    "class HandSignsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset for a hand signs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, transforms):\n",
    "        \"\"\"\n",
    "\n",
    "        initializes the dataset from a dictionary in the format\n",
    "\n",
    "        {\n",
    "            class_0 : list_image_path\n",
    "            ...\n",
    "            class_N : list_image_path\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        self.data = data\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.flat_data = [(img, cls) for cls, imgs in data.items() for img in imgs]\n",
    "\n",
    "    def class_count(self):\n",
    "        \"\"\"\n",
    "        returns the count of data for each class\n",
    "        \"\"\"\n",
    "\n",
    "        for cls, data in self.data.items():\n",
    "\n",
    "            print(f'class {cls} : {len(data)}')\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        returns the\n",
    "        \"\"\"\n",
    "        return len(self.flat_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        indexes self.flat_data, returns a transformed image and the class as a tensor\n",
    "        \"\"\"\n",
    "        image_pth, cls = self.flat_data[index]\n",
    "        image = Image.open(image_pth)\n",
    "        transformed_image = self.transforms(image)\n",
    "\n",
    "        return  transformed_image, torch.tensor(cls)\n",
    "    \n",
    "base_path = Path('./handsign_local')\n",
    "data = extract_images(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c955b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "  def __init__(self, model, train_loader, val_loader, criterion, optimizer, num_epochs=20, scheduler=None):\n",
    "\n",
    "    #training device variables group\n",
    "    self.device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "\n",
    "    self.model = model.to(self.device)\n",
    "\n",
    "    #data loader variables group\n",
    "    self.train_loader = train_loader\n",
    "    self.val_loader = val_loader\n",
    "\n",
    "    #training variables group\n",
    "    self.criterion = criterion\n",
    "    self.optimizer = optimizer\n",
    "    self.num_epochs = num_epochs\n",
    "\n",
    "    self.scheduler = scheduler\n",
    "\n",
    "    #variabels for tracking and plotting\n",
    "    self.train_losses = []\n",
    "    self.val_losses = []\n",
    "    self.train_accuracies = []\n",
    "    self.val_accuracies = []\n",
    "\n",
    "    # TensorBoard writer\n",
    "    self.writer = SummaryWriter()\n",
    "\n",
    "  \"\"\"\n",
    "  This iterates through each epochs to train the model\n",
    "  it tracks train/val loss and accuracy\n",
    "  This methods also write log for tensor board for result visulization and comparation\n",
    "  \"\"\"\n",
    "  def fit(self):\n",
    "    for epoch in range(self.num_epochs):\n",
    "      train_loss, train_accuracy = self.train()\n",
    "      val_loss, val_accuracy = self.evaluate()\n",
    "\n",
    "      if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "      # Store loss and accuracy values\n",
    "      self.train_losses.append(train_loss)\n",
    "      self.train_accuracies.append(train_accuracy)\n",
    "      self.val_losses.append(val_loss)\n",
    "      self.val_accuracies.append(val_accuracy)\n",
    "\n",
    "      # Log metrics to TensorBoard\n",
    "      self.writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "      self.writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "      self.writer.add_scalar('Accuracy/Train', train_accuracy, epoch)\n",
    "      self.writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)\n",
    "      print(f'Epoch [{epoch+1}/{self.num_epochs}], '\n",
    "            f' Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%,'\n",
    "            f' Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "    self.writer.close() # Close the writer after training is complete\n",
    "\n",
    "  \"\"\"\n",
    "  This class mode the model on training mode, and uses running loss, correct predicting, and total samples\n",
    "  to track the loss and accuracy of the model\n",
    "\n",
    "  The main thing of this method is the gradient calculation\n",
    "  This basically adjusting the (w,b), so for example, y = aw+b. We are finding the best set of (w,b)\n",
    "  Then, next time a is enterd, the y will be calculated.\n",
    "\n",
    "  before doing something, we first cleaning the preivous gradient and calculate the new ones\n",
    "  \"\"\"\n",
    "  def train(self):\n",
    "    run_loss = 0.0\n",
    "    correct_pred = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    self.model.train()  # Set model to training mode\n",
    "\n",
    "    for images, labels in self.train_loader:\n",
    "      images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "      self.optimizer.zero_grad()\n",
    "      outputs = self.model(images)\n",
    "      loss = self.criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "      # Track loss and accuracy\n",
    "      run_loss += loss.item()\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      correct_pred += (predicted == labels).sum().item()\n",
    "      total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate training loss and accuracy\n",
    "    train_loss = run_loss / len(self.train_loader)\n",
    "    train_accuracy = correct_pred / total_samples * 100\n",
    "\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "  \"\"\"\n",
    "  This method is used for evaludating the model,\n",
    "  the main role of this one is\n",
    "  after training, we will to evaluate it in order to fine-tune it\n",
    "  Therefore, in this method, there is no gradient calculation\n",
    "  \"\"\"\n",
    "  @torch.no_grad()\n",
    "  def evaluate(self):\n",
    "    val_loss = 0.0\n",
    "    correct_pred = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    self.model.eval()  # Set model to evaluation mode\n",
    "    for images, labels in self.val_loader:\n",
    "      images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = self.model(images)\n",
    "      loss = self.criterion(outputs, labels)\n",
    "      val_loss += loss.item()\n",
    "\n",
    "      # Track accuracy\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      correct_pred += (predicted == labels).sum().item()\n",
    "      total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate validation loss and accuracy\n",
    "    val_loss = val_loss / len(self.val_loader)\n",
    "    val_accuracy = correct_pred / total_samples * 100\n",
    "\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "  def save_model(self, path):\n",
    "    print(f'Saving {self.model} to {path}')\n",
    "    with open(path, 'wb') as f:\n",
    "      pickle.dump(self.model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d232145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader objects created successfully.\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "IMG_size = 299\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    # transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "testVal_transform = transforms.Compose([\n",
    "    transforms.transforms.Resize(IMG_size),\n",
    "    transforms.CenterCrop(IMG_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "train_set = HandSignsDataset(data['train'], train_transform)\n",
    "test_set = HandSignsDataset(data['test'], testVal_transform)\n",
    "val_set = HandSignsDataset(data['valid'], testVal_transform)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(\"DataLoader objects created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2e54761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAoABoDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDms8n7v51NbW73lzFbRhC0jBRnoPeqKsw4H/661NDt7241SBraCZyrjLIhO3646Vm3oaRjdlzxFpUei3kVvHMZi8QdywAw2SCB7cVj7/YfnXReOopItQtJWUgvBt5HcMf8a5TzD/kURd0FTR2PQPhvYxm3luptNR8yERXMuCAB2Xgng969GFxEkqmR1BIwAo4H415R4Pv7VI/L/tO4EKPhYDhPcnPIH/669HtJWmdWtUjcLzukJbd+J61lJu50wV43Ri/ELTl1HRpZkbc9rmWI+gx8w/IZ/CvHfNI/hzXu9zJHeRTwSxhUkDIyr2yMGvNJPAdwJXEdwdgY7eO3ahSsTKBV0/Uzpk3mWccQkAxllqTUPEesTSC4M7K4/wCeR/nRRXbUhG7OWjUlY0NI8UzXZCXM2XHXPGa7RGRo1bzRyAetFFcM4q5303dH/9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=26x40>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_img(path):\n",
    "\n",
    "    return Image.open(path)\n",
    "show_img(data['test'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2b86a9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (Tensor, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (Tensor, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (bool, bool)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (Tensor, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (bool, bool)!, !int!)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m optimizer = optim.Adam(trainable_params, lr=\u001b[32m1e-4\u001b[39m)\n\u001b[32m     13\u001b[39m xception_trainer = Trainer(xception, train_dataloader, val_dataloader, criterion, optimizer, num_epochs=\u001b[32m15\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mxception_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     35\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     train_loss, train_accuracy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     val_loss, val_accuracy = \u001b[38;5;28mself\u001b[39m.evaluate()\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scheduler:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     76\u001b[39m images, labels = images.to(\u001b[38;5;28mself\u001b[39m.device), labels.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     78\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(outputs, labels)\n\u001b[32m     81\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/project/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/project/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mmy_xception.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m   x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mentryflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m   x = \u001b[38;5;28mself\u001b[39m.middleflow(x)\n\u001b[32m     33\u001b[39m   x = \u001b[38;5;28mself\u001b[39m.exitflow(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/project/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/project/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/project/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/project/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/project/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mConvBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.relu(\u001b[38;5;28mself\u001b[39m.bn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/project/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/project/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/project/lib/python3.12/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/project/lib/python3.12/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: conv2d() received an invalid combination of arguments - got (Tensor, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (Tensor, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (bool, bool)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (Tensor, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (bool, bool)!, !int!)\n"
     ]
    }
   ],
   "source": [
    "output_dim = 5\n",
    "\n",
    "xception = my_xception(output_dim, lr=1e-4)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    xception.cuda()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "trainable_params = filter(lambda p: p.requires_grad, xception.parameters())\n",
    "optimizer = optim.Adam(trainable_params, lr=1e-4)\n",
    "\n",
    "xception_trainer = Trainer(xception, train_dataloader, val_dataloader, criterion, optimizer, num_epochs=15)\n",
    "xception_trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
